# Trigo.cpp - High-Performance MCTS Tools

**Project Scope**: C++/CUDA tools for Trigo game engine and MCTS self-play generation

**Goal**: Provide high-performance tools for TrigoRL training pipeline

---

## Architecture

### Component Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Training Pipeline (TrigoRL) - SEPARATE PROJECT      â”‚
â”‚  â”œâ”€ PyTorch Model Training                                   â”‚
â”‚  â”œâ”€ ONNX Model Export (exportOnnx.py)                        â”‚
â”‚  â”œâ”€ Training Data Loading (.tgn files)                       â”‚
â”‚  â””â”€ Weights & Biases Integration                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ exports
                    ONNX Models (.onnx)
                           â†“ uses
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  C++ Inference & Generation Tools (trigo.cpp) - THIS PROJECTâ”‚
â”‚  â”œâ”€ SharedModelInferencer (ONNX Runtime + CUDA)             â”‚
â”‚  â”‚   â”œâ”€ Policy Network Inference                             â”‚
â”‚  â”‚   â”œâ”€ Value Network Inference                              â”‚
â”‚  â”‚   â””â”€ Prefix Tree Attention Builder                        â”‚
â”‚  â”œâ”€ PrefixCacheInferencer (KV Cache Optimization)           â”‚
â”‚  â”‚   â”œâ”€ Two-Stage Inference (prefix + eval)                  â”‚
â”‚  â”‚   â”œâ”€ Persistent Cache Management                          â”‚
â”‚  â”‚   â”œâ”€ Dynamic Shape Support                                â”‚
â”‚  â”‚   â””â”€ 3-5Ã— Speedup for MCTS Pattern                        â”‚
â”‚  â”œâ”€ TrigoGame (3D Go rules engine)                           â”‚
â”‚  â”‚   â”œâ”€ Board State Management                               â”‚
â”‚  â”‚   â”œâ”€ Move Validation                                      â”‚
â”‚  â”‚   â”œâ”€ Capture & Ko Detection                               â”‚
â”‚  â”‚   â””â”€ Territory Calculation                                â”‚
â”‚  â”œâ”€ MCTS (Monte Carlo Tree Search)                           â”‚
â”‚  â”‚   â”œâ”€ Pure MCTS (UCB1, random rollouts)                    â”‚
â”‚  â”‚   â””â”€ AlphaZero MCTS (PUCT, value network) [planned]      â”‚
â”‚  â”œâ”€ Self-Play Generator (data generation tool)               â”‚
â”‚  â”‚   â”œâ”€ RandomPolicy                                         â”‚
â”‚  â”‚   â”œâ”€ NeuralPolicy (ONNX inference)                        â”‚
â”‚  â”‚   â”œâ”€ CachedNeuralPolicy (prefix cache, 3-5Ã— faster)      â”‚
â”‚  â”‚   â”œâ”€ MCTSPolicy                                           â”‚
â”‚  â”‚   â”œâ”€ AlphaZeroPolicy (MCTS + value network)              â”‚
â”‚  â”‚   â””â”€ TGN File Export                                      â”‚
â”‚  â”œâ”€ CUDA Kernels [future]                                    â”‚
â”‚  â”‚   â”œâ”€ Parallel MCTS Tree Operations                        â”‚
â”‚  â”‚   â””â”€ Batched Game State Evaluation                        â”‚
â”‚  â””â”€ Python Bindings (pybind11) [future]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ generates
                    Training Data (.tgn)
                           â†“ feeds back to
                      TrigoRL Pipeline
```

### Project Roles

**TrigoRL** (separate repository):
- PyTorch model training (policy + value networks)
- ONNX model export via `exportOnnx.py`
- Training data loading from .tgn files
- Python-based inference for development

**trigo.cpp** (this repository):
- High-performance C++/CUDA tools for production
- ONNX Runtime inference (CPU + GPU)
- Game engine implementation
- MCTS algorithms (pure + AlphaZero style)
- Self-play data generation (.tgn files)

**Data Flow**:
1. trigoRL trains models â†’ exports .onnx files
2. trigo.cpp loads .onnx â†’ runs self-play â†’ generates .tgn files
3. trigoRL loads .tgn files â†’ continues training (iterative improvement)

---

## Implementation Status

### Phase 1: Model Inference âœ… COMPLETE

**Components**:
- âœ… `SharedModelInferencer` - ONNX Runtime with shared base model
- âœ… `TGNTokenizer` - Compatible with Python training tokenizer
- âœ… `PrefixTreeBuilder` - Tree attention support
- âœ… ONNX models can be loaded and run
- âœ… Model format: 3-model architecture (base + policy_head + value_head)

**Trained Models**:
- Location: `/home/camus/work/trigo.cpp/models/trained_shared/`
- Files: `base_model.onnx`, `policy_head.onnx`, `value_head.onnx`

**Tests**:
- âœ… `test_neural_policy_inference.cpp` - Full inference pipeline
- âœ… `test_shared_model_inferencer.cpp` - Model loading
- âœ… `test_tgn_consistency.cpp` - Format validation

---

### Phase 2: Game Engine âœ… COMPLETE

**Components**:
- âœ… `TrigoGame` - Complete 3D Go engine
- âœ… `trigo_coords.hpp` - ab0yz coordinate encoding
- âœ… `trigo_game_utils.hpp` - Capture, Ko, territory
- âœ… `tgn_utils.hpp` - Shared TGN generation
- âœ… Cross-language validation (100/100 games vs TypeScript)

**Self-Play Generation**:
- âœ… `RandomPolicy` - Baseline
- âœ… `NeuralPolicy` - ONNX inference with correct TGN format
- âœ… `CachedNeuralPolicy` - Prefix cache optimization (3-5Ã— faster for MCTS)
- âœ… `MCTSPolicy` - Basic MCTS (CPU, performance limited)
- âœ… `AlphaZeroPolicy` - MCTS with value network (production-ready)
- âœ… `self_play_generator` - Command-line tool

**Performance**:
- Random vs Random: ~3 games/sec (CPU)
- Neural vs Random: ~1 game/sec (CPU)
- CachedNeural: 3.4Ã— faster than Neural for MCTS pattern
- MCTS vs Random: Too slow (<0.1 games/sec, needs optimization)

**Tests**:
- âœ… `test_trigo_coords.cpp`
- âœ… `test_trigo_game_utils.cpp`
- âœ… `test_trigo_game.cpp`
- âœ… `test_game_replay.cpp`
- âœ… `test_tgn_consistency.cpp`
- âœ… `test_cached_neural_policy.cpp`
- âœ… `test_cached_inference_game.cpp`
- âœ… `benchmark_dynamic_shapes.cpp`

---

### Phase 3: MCTS Algorithm âœ… COMPLETE

**Status**:
- âœ… PureMCTS with random rollouts (`include/mcts_moc.hpp`)
  - UCB1 selection, tree expansion, backpropagation working
  - Reference implementation for validation
  - Performance: ~923ms per simulation (limited to testing)
- âœ… AlphaZero-style MCTS with value network (`include/mcts.hpp`)
  - Uses `SharedModelInferencer::value_inference()` for evaluation
  - PUCT formula for exploration
  - **Performance: 255Ã— speedup** (~3.6ms per simulation vs 923ms)
  - Production-ready implementation

**Performance Comparison**:
| Implementation | Time per simulation | 50 simulations | 800 simulations |
|----------------|---------------------|----------------|-----------------|
| PureMCTS (rollouts) | 923ms | 46 seconds | 12+ minutes |
| MCTS (value network) | 3.6ms | 180ms | 2.9 seconds |
| **Speedup** | **255Ã—** | **255Ã—** | **255Ã—** |

**File Organization**:
- `include/mcts.hpp` - Production MCTS with value network (MCTS class)
- `include/mcts_moc.hpp` - Reference pure MCTS (PureMCTS class)
- `include/self_play_policy.hpp` - Policy interfaces using both implementations

---

### Phase 4: MCTS Performance Benchmarking âœ… COMPLETE

**Completed**: Comprehensive three-way performance comparison (December 5, 2025)

**Test Configuration**:
- 10 games with AlphaZero MCTS (50 simulations per move)
- Board: 5Ã—5Ã—1
- Model: Dynamic ONNX shared architecture
- Hardware: RTX 3090 (24GB), Multi-core CPU

**Performance Results**:

| Implementation | Time per Move | Total Duration | Speedup vs TypeScript |
|----------------|---------------|----------------|----------------------|
| **C++ CPU** | 280ms | 117s | **6.59Ã—** |
| **C++ GPU** | 335ms | 178s | 5.51Ã— |
| **TypeScript** | 1846ms | 641s | 1Ã— (baseline) |

**Key Findings**:

1. **C++ is 5.47Ã— faster than TypeScript** for MCTS self-play
   - Production-ready for large-scale data generation
   - Can generate 10K games in 32.5 hours (CPU)

2. **GPU is SLOWER than CPU for batch=1 MCTS** (counter-intuitive but expected)
   - GPU: 335ms vs CPU: 280ms per move (0.66Ã— performance)
   - **1.52Ã— slower overall** (178s vs 117s)
   - Root cause: Small batch size (batch=1) underutilizes GPU parallelism
   - CUDA kernel launch overhead (~100-150Î¼s) dominates small model inference
   - 7 Memcpy nodes added for GPU, some operators fall back to CPU
   - GPU cores 99% idle with batch=1 workload

3. **GPU advantage depends on workload characteristics**:
   - âŒ Self-play with batch=1 MCTS: CPU wins (1.52Ã— faster)
   - âœ… Training with batch=256+: GPU wins (10-50Ã— faster expected)
   - âœ… Batch inference with 64+ positions: GPU wins (5-20Ã— faster expected)

**Recommendations**:
- **Use CPU for MCTS self-play** (default: `TRIGO_FORCE_CPU=1`)
- Use GPU only for training (where large batches are natural)
- Future optimization: Implement batch MCTS leaf evaluation for GPU
- Future optimization: Parallel self-play (multiple games simultaneously)

**Documentation**: See `docs/PERFORMANCE_ANALYSIS-1205.md` for detailed analysis

---

### Phase 5: KV Cache Optimization âœ… Phases 5.1-5.4 Complete

**Goal**: Implement Prefix KV Cache for BaseModelWithTreeAttention to accelerate MCTS inference

**Status**: Python implementation, ONNX export, and architecture redesign complete (December 8, 2025)

**Research Findings** (documented in `docs/KVCACHE_DESIGN.md`):

âœ… **ONNX Runtime C++ API supports PyTorch-like GPU memory management**
- `IOBinding` API for zero-copy GPU tensor binding
- `Value::CreateTensor()` with CUDA memory for persistent GPU tensors
- Full support for cross-inference tensor reuse

**Implementation Approaches**:

1. **IOBinding + Persistent GPU Tensors** (Recommended)
   - Similar to PyTorch KV cache pattern
   - Zero CPU-GPU copy overhead
   - Highest performance (10-100Ã— speedup for sequential generation)

2. **Manual CUDA Memory Management** (Advanced)
   - Lower-level control with `cudaMalloc`/`cudaFree`
   - Wrap CUDA buffers as `Ort::Value` tensors
   - Suitable for special requirements

**Prototype Validation Results** (documented in `prototype/kvcache/KVCACHE_BENCHMARK.md`):
- âœ… **4.78Ã— speedup** achieved with Python ONNX Runtime
- First 10 tokens: 24.83ms (no cache) â†’ 5.20ms (with cache)
- Average per token: 2.48ms â†’ 0.52ms
- Memory overhead: ~8 KB per token (4-layer model)

**Expected Performance**:
- First token latency: No change
- **Subsequent token latency: 10-100Ã— reduction** (vs recomputing full sequence)
- Memory overhead: ~75 MB per batch (GPT-2 scale, 2048 max length)

**Key APIs**:
```cpp
// Create CUDA memory info
auto memory_info = Ort::MemoryInfo::CreateCuda(device_id, OrtMemTypeDefault);

// Create persistent GPU tensor
Ort::Value cache = Ort::Value::CreateTensor<float>(cuda_allocator, shape);

// Bind to inference
Ort::IoBinding io_binding(session);
io_binding.BindInput("past_key_cache", cache);
io_binding.BindOutput("present_key_cache", memory_info);
```

**Completed Tasks**:

- âœ… **Phase 5.1: Python Core Implementation** (COMPLETE - December 6, 2025)
  - âœ… Modified `BaseModelWithTreeAttention` in `trigoRL/exportOnnx.py`
  - âœ… Implemented two execution modes (cache vs no-cache)
  - âœ… Added cache helper methods (`_get_cache_length`, `_tuple_to_cache`, `_cache_to_tuple`)
  - âœ… Built attention mask builders for both modes
  - âœ… Comprehensive unit tests (12/12 passing)
  - âœ… Integration test with real GPT2 model (all tests passing)
  - âœ… Documentation: See implementation details in `trigoRL/tests/test_kvcache.py`

**Current Tasks**:

- âœ… **Phase 5.2: ONNX Export Implementation** (COMPLETE - December 6, 2025)
  - âœ… Integrated cache export into `export_shared_architecture()` with `with_cache` parameter
  - âœ… Created `CachedONNXWrapper` for flat cache I/O (flattens nested tuple to ONNX-compatible format)
  - âœ… Added `--with-cache` CLI flag to exportOnnx.py
  - âœ… Export functionality: 3 models (no cache) or 4 models (with cache: base_model_cached.onnx)
  - âœ… Validated with onnxruntime: test_kvcache_export_simple.py passes (13.55 MB model, 0.64 ms/iter)
  - âœ… Implementation: Unified into export_shared_architecture() instead of separate function

- âœ… **Phase 5.3: Performance Benchmarking** (COMPLETE - December 6, 2025)
  - âœ… Created benchmark script for trained trigoRL models (tests/benchmark_kvcache.py)
  - âœ… Validated export with 6-layer GPT2 model (3.40 MB base, 3.32 MB cached)
  - âœ… Measured baseline performance: 3.39 ms/sequence (no cache)
  - âš ï¸ **Critical Finding**: Current cache implementation doesn't support MCTS pattern
  - âš ï¸ **Architecture Mismatch**: Cache accumulates (autoregressive) vs MCTS needs fixed prefix reuse
  - âœ… Documented limitation in `docs/KVCACHE_EXPORT_STATUS.md`
  - ğŸ“ **Recommendation**: Redesign cache architecture before C++ integration

- âœ… **Phase 5.4: Architecture Redesign** (COMPLETE - December 8, 2025)
  - âœ… Added three execution modes to BaseModelWithTreeAttention (standard, prefix_only, eval_cached)
  - âœ… Prefix-only mode: Computes prefix â†’ cache
  - âœ… Eval-cached mode: Reuses fixed cache (no updates)
  - âœ… Modified ONNX export to generate 5 models (standard, prefix, eval_cached, policy, value)
  - âœ… Validated MCTS pattern: compute prefix once, reuse for multiple evaluations
  - âœ… Measured speedup: **1.46-1.52Ã— (30-34% faster)**
  - âœ… Comprehensive testing: test_prefix_cache_redesign.py (all passing)
  - âœ… Performance benchmarking: benchmark_prefix_cache_final.py
  - âœ… Documentation: docs/PHASE54_COMPLETE.md

- âœ… **Phase 5.5: C++ Integration** (COMPLETE - December 8, 2025)
  - âœ… Created `PrefixCacheInferencer` class with two-stage API
  - âœ… Implemented cache management (persistent storage, dimension detection)
  - âœ… Comprehensive test suite (basic, MCTS pattern, benchmark)
  - âœ… Performance validation: 18.76ms for 10 evaluations (matches Python)
  - âœ… 10Ã— more stable than Python (Â±0.31ms vs Â±3.08ms)
  - âœ… Documentation: docs/PHASE55_COMPLETE.md
  - ğŸ“ Note: Returns hidden states, not policy logits (design decision)

- âœ… **Phase 5.6: Dynamic Shape Support & Production Integration** (COMPLETE - December 8, 2025)
  - âœ… Added dynamic axes to ONNX export (supports variable prefix/eval lengths)
  - âœ… Created `CachedNeuralPolicy` class integrated with PolicyFactory
  - âœ… GPU support with automatic CPU fallback
  - âœ… Comprehensive performance benchmarking (3 test scenarios)
  - âœ… Performance validation: **3.4Ã— speedup** for MCTS pattern (10 moves)
  - âœ… Dynamic shape overhead: **< 2%** (validated prediction from analysis)
  - âœ… Documentation: docs/PERFORMANCE_ANALYSIS-1208.md, docs/MCTS_PREFIX_CACHE_INTEGRATION.md
  - âœ… Production-ready: Full integration with PolicyFactory, comprehensive testing
  - ğŸ“ **Current Limitation**: Only policy network uses prefix cache
    - Value network in AlphaZero MCTS still uses standard inference (no cache)
    - Each MCTS simulation recomputes prefix for value evaluation
  - ğŸ“ **Key Finding**: Cache is fully shareable between policy and value heads
    - Both heads consume same hidden states from base model
    - Single prefix cache can serve both policy and value inference
    - Potential for 2-3Ã— additional speedup in MCTS

**Implementation Details**:

**Python Core** (`trigoRL/exportOnnx.py:755-1552`):
- `BaseModelWithTreeAttention` redesigned with three execution modes:
  1. **standard**: Full sequence (prefix + evaluated), no cache
  2. **prefix_only**: Compute prefix only â†’ returns cache
  3. **eval_cached**: Evaluate with fixed cache (cache unchanged)
- Mode auto-detection based on inputs if `mode='auto'`
- Cache format: Tuple of ((k_0, v_0), (k_1, v_1), ...) for ONNX compatibility
- Position IDs: Evaluated tokens get positions `prefix_length + mask_row_sums - 1`
- Attention mask: Evaluated tokens attend to full cached prefix

**ONNX Export**:
- Standard mode: 3 models (base, policy, value)
- With cache (`--with-cache`): 5 models
  1. `base_model.onnx` - Standard (no cache)
  2. `base_model_prefix.onnx` - Prefix-only (compute cache)
  3. `base_model_eval_cached.onnx` - Eval-cached (reuse fixed cache)
  4. `policy_head.onnx` - Policy head
  5. `value_head.onnx` - Value head

**Test Coverage**:
- âœ… `test_prefix_cache_redesign.py` - Three-mode functionality
- âœ… `benchmark_prefix_cache_final.py` - Performance validation
- âœ… Numerical consistency: Max diff 0.000001
- âœ… MCTS pattern: Prefix reuse across multiple evaluations
- âœ… Cache verification: Stays fixed (doesn't accumulate)

**Performance**:
- Speedup: **1.46-1.52Ã—** (30-34% faster)
- Test: 6-layer GPT2, prefix=128, eval=64, 10-20 evaluations
- Per evaluation: 1.91 ms (cached) vs 2.91 ms (standard)
- Achieved: 87-91% of theoretical maximum speedup

**Success Criteria**:
- âœ… Three execution modes implemented and validated
- âœ… MCTS prefix-reuse pattern works correctly
- âœ… Speedup achieved: 1.46-1.52Ã— (target was 2Ã—, achieved 87-91% of theoretical max)
- âœ… Cache stays fixed across evaluations (verified)
- âœ… Numerical accuracy excellent (max diff 0.000001)
- âœ… Production-ready ONNX models exported

**Priority**: COMPLETE - C++ integration unblocked

---

### Phase 5.7: Shared Cache for Policy + Value Networks âœ…

**Status**: âœ… **COMPLETE** (December 8, 2025, 15:32 CST)

**Goal**: Enable value network to use prefix cache, sharing the same cache with policy network in AlphaZero MCTS.

**Achievement**: Value network now reuses the same prefix cache as policy network, achieving additional 1.95Ã— speedup over Phase 5.6.

**Implementation Details**:

1. **Added `value_inference_with_cache()` method** (`prefix_cache_inferencer.cpp`)
   - Reuses existing prefix cache for value inference
   - Takes VALUE token (ID=3) as input
   - Returns scalar value prediction [-1, 1]
   - Implementation: 60 lines

2. **Created `CachedAlphaZeroPolicy` class** (`self_play_policy.hpp`)
   - **Note**: Simplified implementation (NOT full MCTS, just proof of concept)
   - Uses value-based greedy selection (no tree search, no simulations)
   - Demonstrates shared cache usage between policy and value heads
   - Integrated with PolicyFactory (type="cached-alphazero")
   - Supports GPU with automatic CPU fallback

3. **Comprehensive Testing**:
   - `test_cached_alphazero_policy.cpp` - Integration validation
   - `benchmark_value_cache_simple.cpp` - Performance measurement
   - `tools/benchmark_cache_comparison.sh` - Comprehensive benchmark suite
   - All tests passed âœ…

**Performance Results** (from comprehensive benchmark):

| Test | Hardware | Description | Prefix Time | Eval Time | Per Eval | Total Time |
|------|----------|-------------|-------------|-----------|----------|------------|
| Value Cache (2 moves) | CPU | 23 tokens, 10 evals | 1.08 ms | 7.81 ms | **0.78 ms** | 8.89 ms |
| Value Cache (4 moves) | CPU | 32 tokens, 10 evals | 1.14 ms | 8.24 ms | **0.82 ms** | 9.38 ms |
| Value Cache (6 moves) | CPU | 41 tokens, 10 evals | 1.32 ms | 8.53 ms | **0.85 ms** | 9.84 ms |
| CachedAlphaZero (avg) | GPU | 10 selections | - | - | - | **5.21 ms** |
| Real Game (5 moves) | CPU | 32 tokens | 1.93 ms | 2.11 ms | **0.42 ms** | 4.04 ms |

**Key Metrics**:
- Value inference with cache: **0.42-0.85 ms** per evaluation (CPU)
- Prefix computation: **1.08-1.93 ms** (one-time cost)
- CachedAlphaZeroPolicy: **5.21 ms** average (GPU, after warmup)
- **2.43Ã— speedup** for MCTS value pattern vs standard inference

**Phase-by-Phase Improvements**:
- Phase 5.6 (policy only): 25.8ms total
- Phase 5.7 (policy + value): 13.2ms total
- **Additional speedup: 1.95Ã—** (49% reduction)

**Overall Performance Evolution**:
- Original TypeScript: 1846 ms per move (baseline)
- Phase 4 (C++ base): 280 ms (6.59Ã—)
- Phase 5.6 (policy cache): ~200 ms (9.23Ã—)
- **Phase 5.7 (policy + value cache): ~150 ms (~12.3Ã—)**
- **Combined: ~12-13Ã— faster than TypeScript**

**Documentation**:
- `docs/PERFORMANCE_ANALYSIS-1208.md` - Updated with Phase 5.7 section and comprehensive benchmark results
- `tools/benchmark_cache_comparison.sh` - Comprehensive benchmark script

**Production Readiness**:
- âœ… Core functionality complete and tested
- âœ… Integration with PolicyFactory
- âœ… Comprehensive performance validation
- âœ… Cache sharing validated (no additional memory overhead)
- âš ï¸ **Limitation**: Current CachedAlphaZeroPolicy is simplified (no tree search)
- â­ï¸ **Next Step**: Full MCTS integration required for production AlphaZero (see `docs/FULL_MCTS_CACHE_TODO.md`)

---
## Phase 5.8: C++ vs TypeScript MCTS Consistency âœ… COMPLETE

**Status**: All HIGH/MEDIUM/LOW priority items complete (December 10, 2025)

**Goal**: Align C++ `cached_mcts.hpp` with TypeScript `mctsAgent.ts` to ensure consistent move selection.

**Background**: GPT-5.1 comprehensive review (December 10, 2025) identified several behavioral differences between the two implementations that could cause divergent game play.

### Identified Differences

#### 1. Terminal State Detection & Valuation âœ… COMPLETE

| Aspect | TypeScript | C++ |
|--------|------------|-----|
| Detection | `checkTerminal()` with two conditions | âœ… `checkTerminal()` added |
| Value source | Ground-truth territory calculation | âœ… Ground-truth territory |
| Formula | `sign(diff) * (1 + log(|diff|))` | âœ… Same formula |

**TypeScript behavior**:
```typescript
// checkTerminal() checks:
// 1. gameStatus === "finished" (double-pass, resignation)
// 2. coverage > 50% AND neutral === 0 (natural end)
// Returns: calculateTerminalValue(territory) = sign(scoreDiff) * (1 + log(|scoreDiff|))
```

**C++ behavior** (UPDATED December 10, 2025):
```cpp
// checkTerminal() now checks same conditions as TypeScript:
// 1. get_game_status() == GameStatus::FINISHED
// 2. coverageRatio > 0.5f && territory.neutral == 0
// Returns: calculateTerminalValue(territory) with same formula
auto terminal_value = checkTerminal(game_copy);
if (terminal_value.has_value()) {
    value = terminal_value.value();  // Ground-truth
} else {
    value = evaluate_with_cache(game_copy);  // NN inference
}
```

**Fix Applied**:
- [x] Added `checkTerminal()` function to C++ with same logic as TypeScript
- [x] Added `calculateTerminalValue()` using territory-based formula
- [x] Modified `search()` to skip NN inference when terminal, use ground-truth value
- [x] Validated with test_terminal_detection.cpp (all tests pass)

**GPT-5.1 Review Notes**:
- âœ… Terminal detection logic matches TypeScript exactly
- âœ… Formula `sign(scoreDiff) * (1 + log(|scoreDiff|))` is identical
- âœ… Value sign convention (white-positive) consistent with `evaluate_with_cache()` and `backpropagate()`
- âœ… Coverage > 0.5 threshold and neutral == 0 check match TS
- Note: `get_territory()` is non-const (may update internal cache), safe for repeated calls

---

#### 2. Zero-Prior Move Penalty âœ… COMPLETE

| Aspect | TypeScript | C++ |
|--------|------------|-----|
| Handling | No penalty, Q alone can drive selection | âœ… No penalty (removed) |

**C++ code** (`select_best_puct_child`) - UPDATED December 10, 2025:
```cpp
// Black minimizes Q (flips sign), White maximizes Q
float score = (is_white ? q : -q) + u;

// NOTE: Removed -1000 penalty for zero-prior moves (December 10, 2025)
// TypeScript mctsAgent.ts does NOT have this penalty.
// Allowing Q to drive selection even for low-prior moves is consistent with
// AlphaZero behavior where value network can override policy network.
```

**Fix Applied**:
- [x] Removed `-1000` penalty in `select_best_puct_child()`
- [x] Verified Pass move (prior=0.000000) can now be visited (visits=1 in test)
- [x] No regression in normal play (test_mcts_full_search passes)

**GPT-5.1 Review Notes**:
- âœ… Now matches TypeScript and AlphaZero-style PUCT exactly
- P=0 moves have U=0, so they're explored only when Q becomes attractive (expected behavior)
- Policy shapes exploration but doesn't absolutely forbid low/zero-prior moves
- If stronger exploration of P=0 moves is needed, could add `min_prior` floor (but would deviate from TS)

---

#### 3. Expansion First-Child Selection âœ… COMPLETE

| Aspect | TypeScript | C++ |
|--------|------------|-----|
| Selection | Deterministic PUCT (all N=0 initially) | âœ… Deterministic highest-prior |

**C++ code** (`expand()`) - UPDATED December 10, 2025:
```cpp
// Select the first child to traverse: use highest prior (deterministic)
// TypeScript consistency: After expansion, select() uses PUCT which picks highest P when all N=0
// This is equivalent to picking the child with highest prior deterministically
size_t best_idx = 0;
float best_prior = node->children[0]->prior_prob;
for (size_t i = 1; i < node->children.size(); i++)
{
    if (node->children[i]->prior_prob > best_prior)
    {
        best_prior = node->children[i]->prior_prob;
        best_idx = i;
    }
}
MCTSNode* selected_child = node->children[best_idx].get();
```

**Fix Applied**:
- [x] Replaced `std::discrete_distribution` (prior-weighted random) with deterministic highest-prior selection
- [x] Now matches TypeScript behavior: after expand, PUCT with all N=0 picks highest P
- [x] Verified highest-prior move (0z, prior=0.196) gets more visits (7 vs 5 before)

**GPT-5.1 Review Notes**:
- âœ… Behavior matches TypeScript: PUCT with N=0 gives score = c*P, so highest P wins
- âœ… Moves closer to canonical AlphaZero behavior (expansion sets priors, selection is pure PUCT)
- âœ… Reduces randomness in first rollout after expansion â†’ better reproducibility
- Uses `>` comparison, so first child wins ties (same as TS iteration order)

---

#### 4. Root Visit Count Initialization âœ… COMPLETE

| Aspect | TypeScript | C++ |
|--------|------------|-----|
| Initial value | `totalN = 0` (sum of child N values) | âœ… `visit_count = 0` (default) |

**C++ code** (`search()`) - UPDATED December 10, 2025:
```cpp
// Create root node
root = std::make_unique<MCTSNode>(Position{0, 0, 0}, false);
// NOTE: root->visit_count defaults to 0 (matches TypeScript totalN=0 initially)
// TypeScript: totalN = sum of all child N values, starts at 0
// PUCT uses sqrt(totalN + 1), so initial U = c * P * sqrt(1) / 1 = c * P
```

**Fix Applied**:
- [x] Removed `root->visit_count = 1` initialization
- [x] Root now defaults to visit_count=0 (matches TypeScript totalN=0)
- [x] Initial U term now matches: `sqrt(0 + 1) / 1 = 1` instead of `sqrt(1 + 1) / 1 = 1.414`

**GPT-5.1 Review Notes**:
- âœ… Mathematically consistent with PUCT formula
- âœ… Removes off-by-sqrt(2) mismatch at root
- âœ… Backprop will naturally increment root from 0â†’1 on first simulation
- First backup now sets root 0â†’1 instead of 1â†’2 (standard MCTS behavior)

---

#### 5. Temperature-based Move Selection âœ… COMPLETE

| Aspect | TypeScript | C++ |
|--------|------------|-----|
| Support | Temperature sampling for training | âœ… Temperature sampling supported |

**C++ code** (`search()` and `select_best_child()`) - UPDATED December 10, 2025:
```cpp
// search() now accepts temperature parameter
PolicyAction search(const TrigoGame& game, float temperature = 0.0f)

// select_best_child() implements temperature-based sampling
// Ï€(a|s) âˆ N(s,a)^(1/Ï„)
if (temperature < 0.01f) {
    // Greedy argmax (deterministic)
} else {
    // Sample from N^(1/Ï„) distribution
    float n_pow = std::pow(child->visit_count, 1.0f / temperature);
}
```

**Fix Applied**:
- [x] Added `temperature` parameter to `search()` (default 0.0 for backward compatibility)
- [x] Implemented temperature-based sampling in `select_best_child()`
- [x] Matches TypeScript formula: `Ï€(a|s) âˆ N(s,a)^(1/Ï„)`
- [x] Edge case handling: fallback to uniform random if sum is invalid

**GPT-5.1 Review Notes**:
- âœ… Same cutoff (`< 0.01`) as TypeScript
- âœ… Same `N^(1/Ï„)` transformation
- âœ… Same categorical sampling procedure
- âœ… Fallback behavior for pathological sumN matches TS
- âœ… Default temperature=0.0 reproduces old greedy behavior

---

### Consistent Aspects âœ…

These aspects are already aligned between implementations:

| Aspect | Status |
|--------|--------|
| Value Convention | âœ… Both white-positive, no backup sign flip |
| PUCT Base Formula | âœ… `(isWhite ? Q : -Q) + cPuct * P * sqrt(N+1)/(1+n)` |
| Policy Priors | âœ… Both use log scores + softmax |
| Dirichlet Noise | âœ… Î±=0.03, Îµ=0.25, same timing |
| Node Expansion | âœ… AlphaZero style (all children at once) |
| Backup | âœ… White-positive, no sign flip |

---

### Implementation Plan

**Phase 5.8.1**: Terminal Detection âœ… COMPLETE (December 10, 2025)
1. âœ… Added `checkTerminal()` to `cached_mcts.hpp`
2. âœ… Added `calculateTerminalValue()` with log-scaled territory formula
3. âœ… Modified `search()` to use ground-truth value at terminal states
4. âœ… Tested with `test_terminal_detection.cpp` - all tests pass

**Phase 5.8.2**: Zero-Prior Handling âœ… COMPLETE (December 10, 2025)
1. âœ… Removed `-1000` penalty in `select_best_puct_child()`
2. âœ… Verified low-prior moves can be selected (Pass with prior=0 got visits=1)
3. âœ… No regression in normal play

**Phase 5.8.3**: Minor Alignments âœ… COMPLETE (December 10, 2025)
1. âœ… Aligned first-child selection: deterministic highest-prior instead of random sampling
2. âœ… Aligned root visit count: init to 0 instead of 1 (matches TypeScript totalN=0)
3. âœ… Added temperature support: `search(game, temperature)` with N^(1/Ï„) sampling

**Estimated Effort**: 2-4 hours for Phase 5.8.1-5.8.2

---

### Validation

After fixes, validate consistency:
1. Run both implementations on same game states
2. Compare move selection with same NN weights
3. Compare search statistics (visit counts, Q values)
4. Run tournament between C++ and TypeScript engines

---

## Phase 5.9: MCTS Expansion Strategy Fix âœ… COMPLETE

**Status**: âœ… COMPLETE (December 11, 2025)

**Goal**: Fix C++ MCTS expansion strategy to match TypeScript AlphaZero behavior.

**Background**: GPT-5.1 and Gemini-3-Pro code review identified that C++ used "Traditional MCTS" expansion while TypeScript used "AlphaZero-style" expansion.

### Problem Identified

**Traditional MCTS (C++ before fix)**:
```cpp
// Forced visiting every child with visit_count==0 before using PUCT
for (child : node->children) {
    if (child->visit_count == 0) {
        return child;  // Wrong: ignores policy priors!
    }
}
// Then use PUCT
```

**AlphaZero-style (TypeScript / correct)**:
```typescript
// Use PUCT immediately after expansion
// When all N=0, PUCT score = c * P * sqrt(1) / 1 = c * P
// So highest-prior child is selected (policy network guides exploration)
```

### Issues Caused

1. **Policy network guidance ignored** for first K simulations (K = number of children)
2. **Dirichlet noise ineffective** during forced expansion phase

### Fix Applied

Modified `cached_mcts.hpp`:

1. **`expand()` function**: Mark node as `is_fully_expanded = true` immediately after creating children (don't return a child directly)

2. **Main simulation loop**: After expansion, use PUCT to select child:
```cpp
if (game_copy.is_game_active() && !node->is_fully_expanded)
{
    expand(node, game_copy);

    // Apply Dirichlet noise after first expansion
    if (!dirichlet_applied && !root->children.empty())
    {
        add_dirichlet_noise_to_root();
        dirichlet_applied = true;
    }

    // Use PUCT to select child (key change!)
    if (!node->children.empty())
    {
        bool is_white = (game_copy.get_current_player() == Stone::White);
        node = select_best_puct_child(node, is_white);
        // Apply selected move...
    }
}
```

### Performance Impact

| Metric | Before (Dec 5) | After (Dec 11) | Change |
|--------|----------------|----------------|--------|
| Total Duration | 117.1s | 93s | **-20.6%** |
| Games/sec | 0.085 | 0.108 | **+26.5%** |
| Time/Move | 280ms | 333ms | +18.9% |
| Avg Moves/Game | 41.8 | 27.9 | -33.3% |

**Analysis**:
- Per-move time increased (correct algorithm has more overhead)
- But games are shorter (better play quality with proper policy guidance)
- Overall throughput improved by 26.5%

### Additional Fix: Model Loading Optimization

Also fixed `self_play_generator.cpp` to load ONNX models once instead of per-game:

```cpp
// Before: Models loaded for each game
void generate_one_game(int game_id) {
    auto black = PolicyFactory::create(...);  // Load models!
    auto white = PolicyFactory::create(...);  // Load models!
}

// After: Models loaded once
void generate() {
    auto black = PolicyFactory::create(...);  // Load once
    auto white = PolicyFactory::create(...);  // Load once

    for (int i = 0; i < num_games; i++) {
        generate_one_game(i, black.get(), white.get());
    }
}
```

### Documentation

- `docs/PERFORMANCE_ANALYSIS-1211.md` - Full benchmark results

---

## Phase 5.10: Incremental KV Cache for Self-Play - COMPLETE

**Status**: âœ… COMPLETE (December 11, 2025)

**Goal**: Implement incremental KV cache management for the entire self-play process.

### Problem Identified

Current implementation **recomputes prefix cache on every call**, defeating the purpose:

```cpp
// In evaluate_with_cache() - WRONG:
float evaluate_with_cache(TrigoGame& game) {
    auto tokens = game_to_tokens(game);  // Full sequence
    inferencer->compute_prefix_cache(tokens, 1, seq_len);  // Recomputes EVERY time!
    return inferencer->value_inference_with_cache(3);
}
```

**Performance Impact (Bug)**:
- `cached-mcts` is 2Ã— SLOWER than `alphazero` because cache is recomputed

### Solution: New `eval_extend` Mode

Added a 4th mode to support incremental cache extension:
- Takes existing cache + new tokens
- Returns hidden_states AND updated cache (with new KV appended)

### Implementation Progress - ALL STEPS COMPLETE

**Step 1: Python - Add `eval_extend` mode** âœ… COMPLETE
- Added `eval_extend` mode to `BaseModelWithTreeAttention`
- Mode uses `use_cache=True` to return updated cache
- Returns `(hidden_states, new_cache)` tuple

**Step 2: Python - Export ONNX model** âœ… COMPLETE
- Added `EvalExtendWrapper` class
- Exports `base_model_eval_extend.onnx`
- With cache: now exports 6 models

**Step 3: Python - Equivalence Test** âœ… COMPLETE
- File: `/home/camus/work/trigoRL/tests/test_eval_extend_equivalence.py`
- All tests pass âœ…

**Step 4: C++ - Add `extend_cache()` method** âœ… COMPLETE
- Implemented in `prefix_cache_inferencer.cpp`
- Test: `test_extend_cache.cpp` passes âœ…

**Step 5: C++ - Create `IncrementalCachedMCTSPolicy`** âœ… COMPLETE
- Added `IncrementalCachedMCTSPolicy` class to `self_play_policy.hpp`
- Extends cache between moves in a game
- Available via `--black-policy incremental-mcts`

**Step 6: Benchmark and validate** âœ… COMPLETE

**Step 7: Consistency Bug Fix** âœ… COMPLETE (December 11, 2025)
- **Bug 1**: `eval_extend` returned cache with dummy token included
  - Fix: Strip dummy token from returned cache in Python
- **Bug 2**: New tokens attended to dummy token in `eval_extend` mode
  - Fix: Modified attention mask to skip dummy token
- **Test**: `test_incremental_consistency.cpp` now passes with 0 difference
- All 6 states tested, all differences = 0.000000

### Benchmark Results (December 11, 2025)

| Policy | Time (3 games) | Time/Game | vs alphazero |
|--------|---------------|-----------|--------------|
| **alphazero** | 12s | 4s | 1Ã— (baseline) |
| **cached-mcts** | 216s | 72s | 18Ã— slower |
| **incremental-mcts** | 278s | 93s | 23Ã— slower |

### Key Finding: Prefix Cache Doesn't Help MCTS

The prefix cache approach **fundamentally conflicts with MCTS tree search**:

1. **Within MCTS**: Each simulation explores a different branch
   - Different branches need different cache states
   - Cache can't be shared across branches
   - Must recompute prefix for each branch

2. **Between moves**: Cache can be extended incrementally
   - But within each MCTS search, we still recompute for each branch
   - The incremental benefit is minimal (~5%) vs per-search overhead

3. **Overhead**: PrefixCacheInferencer has more overhead than SharedModelInferencer
   - Multiple model loading (prefix + eval_cached + eval_extend)
   - Cache management complexity
   - No actual cache reuse within MCTS

### Conclusion

The prefix cache optimization as implemented is **NOT suitable for MCTS**.

**Recommendation**: Use `alphazero` policy (SharedModelInferencer) for production.

**Future Work** (if pursuing cache optimization):
- Batch leaf evaluation: Collect all leaves, evaluate in batch
- Tree-based caching: Store cache states per node (high memory cost)
- Different search algorithm: Beam search (linear) instead of MCTS (tree)

### Files Modified

**Python (trigoRL)**:
1. `exportOnnx.py` - Added eval_extend mode and EvalExtendWrapper, fixed cache/attention bugs
2. `tests/test_eval_extend_equivalence.py` - Equivalence tests

**C++ (trigo.cpp)**:
3. `include/prefix_cache_inferencer.hpp` - Added extend_cache()
4. `src/prefix_cache_inferencer.cpp` - Implemented extend_cache()
5. `include/self_play_policy.hpp` - Added IncrementalCachedMCTSPolicy
6. `tests/test_incremental_consistency.cpp` - Consistency validation test
6. `tests/test_extend_cache.cpp` - C++ test

---


---

### Phase 6: Batched GPU Acceleration - FUTURE

**Planned Components**:
- Batch MCTS leaf evaluation (evaluate 64-256 positions simultaneously)
- Parallel self-play generation (8-16 games concurrently)
- CUDA MCTS kernels for parallel tree operations
- Target: 10-20Ã— speedup with proper batching

**Priority**: Lower (single-game performance already excellent after Phase 5)

**Not Started**.

---

### Future Research: Batch MCTS with Shared Prefix Cache

**Status**: Not Started (Research Direction)

**Motivation**:
- Current MCTS evaluates leaves one-by-one (batch=1)
- GPU is severely underutilized at batch=1 (99% idle)
- KV cache doesn't help because each branch needs different cache state
- Batching leaf evaluations could leverage GPU parallelism

**Approach**:
```
Standard MCTS (current):
  For each simulation:
    1. Select leaf
    2. Evaluate (batch=1) â†’ 3-5ms
    3. Backpropagate

Batch MCTS (proposed):
  1. Run N simulations in parallel, each selects a leaf
  2. Collect all N leaves (may share some prefix)
  3. Batch evaluate all leaves together (batch=N) â†’ 5-10ms total
  4. Backpropagate all N results

With shared prefix optimization:
  1. Group leaves by common prefix
  2. Compute prefix cache once per group
  3. Batch evaluate suffixes with shared cache
```

**Implementation Tasks**:
1. Modify MCTS to support "virtual loss" for parallel tree traversal
2. Implement leaf collection phase (run N selections without immediate evaluation)
3. Group leaves by common prefix (trie-based grouping)
4. Batch inference with variable-length sequences (padding or bucketing)
5. Parallel backpropagation

**Expected Performance**:
- N=64 leaves per batch: ~10ms total (vs 64Ã—5ms = 320ms sequential)
- **32Ã— speedup** for leaf evaluation phase
- Overall MCTS speedup: 10-20Ã— (evaluation dominates time)

**Challenges**:
- Virtual loss tuning (affects exploration vs exploitation)
- Memory for multiple game state copies
- Prefix grouping efficiency
- Variable-length batching overhead

**Priority**: Medium (high impact, moderate complexity)

---

### Future Research: MuZero-Style Architecture

**Status**: Not Started (Research Direction)

**Motivation**:
- Current architecture: variable-length token sequences â†’ transformer â†’ KV cache
- KV cache grows with game length, doesn't fit MCTS tree structure
- MuZero uses fixed-size hidden states, avoids sequence length issues

**MuZero Architecture Overview**:
```
Three Networks:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Representation Network (h)                               â”‚
â”‚    observation â†’ hidden_state (fixed size, e.g., 256-dim)   â”‚
â”‚    Runs once per real game state                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Dynamics Network (g)                                     â”‚
â”‚    (hidden_state, action) â†’ (next_hidden_state, reward)     â”‚
â”‚    Runs many times during MCTS (imagined rollouts)          â”‚
â”‚    FIXED computation cost per step (no growing cache!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Prediction Network (f)                                   â”‚
â”‚    hidden_state â†’ (policy, value)                           â”‚
â”‚    Runs at each MCTS node for expansion/evaluation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MCTS with MuZero:
  Real state â†’ h() â†’ hidden_0
  For each simulation:
    hidden = hidden_0
    For depth 1..K:
      action = select(hidden)
      hidden, reward = g(hidden, action)  # Constant time!
      policy, value = f(hidden)
    Backpropagate
```

**Key Advantages for Trigo**:
1. **Fixed computation per step**: No KV cache, no growing sequences
2. **Perfect for MCTS**: Hidden states are fixed-size, easy to store per node
3. **Learned dynamics**: Model learns game rules implicitly
4. **Planning in latent space**: Can plan beyond observed states

**Implementation Tasks**:
1. Design hidden state representation for 3D Go (CNN + MLP or transformer encoder)
2. Implement dynamics network (action embedding + hidden state â†’ next hidden)
3. Implement prediction network (hidden â†’ policy logits + value)
4. Modify MCTS to use learned dynamics instead of game engine
5. Training: combine policy loss, value loss, and reward prediction loss
6. Handle reward prediction (captures, territory changes)

**Architecture Options**:
- **Option A: CNN-based** (like original MuZero for Atari/Go)
  - Representation: 3D CNN on board state â†’ 256-dim
  - Dynamics: MLP(hidden_256 + action_one_hot) â†’ hidden_256
  - Prediction: MLP(hidden_256) â†’ (policy, value)

- **Option B: Transformer-based** (hybrid approach)
  - Representation: Transformer encoder on board positions â†’ 256-dim CLS token
  - Dynamics: Cross-attention(hidden, action_embedding) â†’ next_hidden
  - Prediction: MLP head on hidden

**Expected Performance**:
- Dynamics call: ~0.1ms (small MLP, vs 3-5ms for full transformer)
- 50 simulations Ã— 10 depth: 500 dynamics calls â†’ 50ms total
- **10Ã— faster** than current MCTS with neural evaluation

**Challenges**:
- Training stability (MuZero is complex to train)
- Hidden state capacity (must encode enough information)
- Reward prediction for Go (sparse, delayed rewards)
- Generalization to unseen positions

**Priority**: Lower (high complexity, requires significant architecture change)

**References**:
- [MuZero Paper](https://arxiv.org/abs/1911.08265) - DeepMind, 2019
- [EfficientZero](https://arxiv.org/abs/2111.00210) - Sample-efficient MuZero variant

## Development Guidelines

### Code Style
- C++17 standard
- Modern C++ (curly braces on standalone lines, tab indentation)
- Comprehensive comments
- DRY principle (avoid code duplication)

### Testing
- Unit tests for each component
- Cross-language validation where applicable
- Performance regression tests

### Focus
- **This project**: Tools and infrastructure
- **TrigoRL project**: Training, model export, Python training pipeline
- No training code in trigo.cpp

---

## References

### TypeScript Source (for validation)
- `trigoRL/third_party/trigo/trigo-web/inc/trigo/game.ts`
- `trigoRL/third_party/trigo/trigo-web/inc/trigo/gameUtils.ts`

### Python Integration
- `trigoRL/trigor/data/tgn_dataset.py` - Loads .tgn files
- `trigoRL/trigor/data/tokenizer.py` - TGN tokenization
- `trigoRL/exportOnnx.py` - ONNX model export

---

**Last Updated**: December 11, 2025

**Current Status**:
- Phase 4 MCTS Benchmarking complete - C++ CPU is 6.59Ã— faster than TypeScript
- Phase 5 KV Cache (5.1-5.6) complete - Full stack from Python to C++ production-ready
- Phase 5.7 complete - Shared cache for policy + value networks
- Phase 5.8 complete - C++ vs TypeScript MCTS consistency
- Phase 5.9 complete - MCTS expansion strategy fix (AlphaZero-style)
- **Phase 5.10 COMPLETE** - Incremental cache implementation done, but benchmark shows it's slower

**Key Finding (Dec 11, 2025)**:
- Prefix cache approach fundamentally conflicts with MCTS tree search
- `cached-mcts` is 18Ã— SLOWER than `alphazero` due to per-branch cache overhead
- `incremental-mcts` is 23Ã— SLOWER (additional eval_extend model overhead)
- Root cause: MCTS explores many branches, each needs different cache state

**Production Ready**: Use `alphazero` policy for all production workloads

**Overall Performance**:
- Original TypeScript: 1846 ms per move (baseline)
- **C++ alphazero (CPU)**: ~140 ms per move (**13Ã— speedup** - BEST)
- C++ cached-mcts: ~2600 ms per move (0.7Ã— - SLOWER than TS!)
- C++ incremental-mcts: ~3350 ms per move (0.5Ã— - SLOWEST)

**Next Steps**:
- Deprecate `cached-mcts` and `incremental-mcts` policies
- Focus on batch/parallel optimizations for `alphazero` if needed

---
